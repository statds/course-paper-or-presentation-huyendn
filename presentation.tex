\documentclass{beamer}
\mode<presentation>{
	\usetheme{Madrid}
}

\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}

\usepackage{tikz}
\usepackage{amsmath}
\usetikzlibrary{matrix}

\title[University of Connecticut] {Hidden Markov Models in Biological Sequence Analysis}

\author{Huyen D. Nguyen}

\institute[]{University of Connecticut \\
	\medskip
	\text{Department of Statistics}\\
}
\date{November 06, 2023}

\begin{document}
\begin{frame}
	\titlepage
\end{frame}

\begin{frame}{Introduction: Hidden Markov Model}
	\begin{itemize}
		\item A \textbf{hidden Markov Model (HMM)} is a statistical model that can be used to describe the evolution of observable events that depend on a Markov process with unobservable states. 
		\item Many real world problems deal with classifying raw observations into a number of categories or class labels that are more meaningful.
		\item HMM applications in signal processing, pattern recognition, economics, finance, bioinformatics, etc.
	\end{itemize}
\end{frame}

\begin{frame}{Introduction: Biological Sequence Analysis}
	\begin{itemize}
		\item The modeling approach of HMM is useful in modeling biological sequencess.
		\item Example: Proteins generally consists of multiple domains \cite{yoon2009hidden}. 
		\begin{enumerate}
			\item Predict the constituting domains (one or more hidden Markov states) and their location in amino acid sequences (observations).
			\item Find the protein family that the a protein sequence belongs.
		\end{enumerate}
	\end{itemize}
\end{frame}

\begin{frame}{Outline}
	\tableofcontents
\end{frame}

\section{Overview of Hidden Markov Model}
\begin{frame}{HMM Definition}
	\begin{itemize}
		\item A set of $N$  hidden states, $ Q = q_1 q_2\cdots q_N.$
		\item A transition probability matrix $A$, each $a_{ij}$ representing the probability of moving state $i$ to state $j$, s.t $\sum_{j=1}^{N} a_{ij} = 1 \forall i$.
		\item A sequence of $T$ observations, $ O = o_1 o_2\cdots o_T$, each one drawn from a set $V = v_1, v_2, \cdots, v_V$.
		\item A sequence of emission probabilities, $B = b_i(o_t)$,  each expressing the probability of an observation $o_t$ being generated from state $i$.
		\item An initial probability distribution over states, $\pi = \pi_1, \pi_2,\cdots, \pi_N$. $\pi_i$ is the probability that the Markov chain will start in state $i$. $\sum_{i=1}^{N} \pi_i = 1$.
	\end{itemize}
\end{frame}

\begin{frame}{HMM Definition}
	\begin{itemize}
		\item Markov Assumption: the probability of a particular state depends only on the previous state
		\begin{equation}
			\begin{split}
				P(q_i \vert q_1\cdots q_{i-1}) = P(q_i \vert q_{i-1}).
			\end{split}
		\end{equation}
		\item Output Independence: the probability of an output observation $o_i$ depends only on the state that produced the observation $q_i$ and not on previous states or observations
		\begin{equation}
			\begin{split}
				P(o_i \vert q_1,\cdots q_T, o_1,\cdots , o_T) = P(o_i \vert q_i) .
			\end{split}
		\end{equation}
	\end{itemize}
	\begin{figure}
		\centering
		\resizebox{3in}{!}{\begin{tikzpicture}
				\matrix[matrix of math nodes,column sep=2em,row
				sep=4em,cells={nodes={circle,draw,minimum width=3em,inner sep=0pt}},
				column 1/.style={nodes={rectangle,draw=none}},
				column 5/.style={nodes={rectangle,draw=none}},
				ampersand replacement=\&] (m) {
					\text{Markov process}: \&
					q_1 \& q_2 \& q_3 \& \cdots \& q_{T}\\
					\text{Observations}: \& 
					o_1 \& o_2 \& o_3 \& \cdots \& o_{T}\\
				};
				\foreach \X in {2,3,4,5}
				{\draw[-latex] (m-1-\X) -- (m-1-\the\numexpr\X+1) node[midway,above]{$a$};
					\ifnum\X=5
					\draw[-latex] (m-1-6) -- (m-2-6) node[pos=0.6,left]{$b$};
					\else
					\draw[-latex] (m-1-\X) -- (m-2-\X) node[pos=0.6,left]{$b$};
					\fi}
				\draw[dashed] ([yshift=1ex]m.east) -- ([yshift=1ex]m.east-|m-1-1.east);
		\end{tikzpicture}}
		\caption{An example of a hidden Markov model.}
		\label{fig:HiddenMarkov}
	\end{figure}
\end{frame}
\section{The Scoring Problem and the Forward Algorithm}
\begin{frame}{Outline}
	\tableofcontents[currentsection]
\end{frame}
\begin{frame}{The Scoring Problem and the Forward Algorithm}
	\begin{block}{The Scoring Problem}
		Given a HMM $\lambda = (A,B)$ and a sequence of observation $O$, what is the probability of observing the sequence $P(O \vert \lambda)$?
	\end{block}
	\begin{itemize}
		\item We can compute the probability of the observations by summing over all possible hidden state sequences:
		$$P(O) = \sum_Q P(O, Q) = \sum_Q P(O\vert Q) P(Q).$$
		\item For an HMM with $N$ hidden states and an observation sequence of $T$ observations, there are $N^T$ possible hidden sequences.
		\item $N^T$ can get very large.
	\end{itemize}
\end{frame}

\begin{frame}{The Forward Algorithm}
	\begin{itemize}
		\item The forward algorithm is a dynamic programming algorithm. 
		\item The probability of being in state $j$ knowing the first $t$ observaitons, given the model $\lambda$
		\begin{equation}
			\alpha_t(j) = P(o_1\cdots o_t, q_t = j \vert \lambda).
		\end{equation}
		\item $\alpha_t(j)$ is computed recursively by summing over the extensions of all the paths that lead to the current cell
		\begin{equation}
			\alpha_t(j) = \sum_{i=1}^N \alpha_{t-1}(i) a_{ij} b_j (o_t).
		\end{equation}
		\item The probability of observing the sequence
		\begin{equation}
			P(O \vert \lambda) = \sum_{i=1}^N \alpha_T(i).
		\end{equation}
	\end{itemize}
\end{frame}

\begin{frame}{Backward Algorithm}
	\begin{itemize}
		\item The probability of seeing observations from $t+1$ to the end, given that we are in state $i$ at time $t$
		\begin{equation}
			\beta_t(i) = P(o_{t+1}o_{t+2}\cdots o_T q_t = i \vert \lambda).
		\end{equation}
		\item It can be computed in a similar way as the forward probability
		\begin{equation}
			\beta_t(i) = \sum_{j=1}^N a_{ij}b_j(o_{t+1})\beta_{t+1}(j), \quad 1 \leq i \leq N, 1 \leq t < N.
		\end{equation}
		\item The probability of observing the sequence
		\begin{equation}
			P(O \vert \lambda) = \sum_{j=1}^N \pi_j b_j(o_1) \beta_1(j).
		\end{equation}
	\end{itemize}
\end{frame}
%\begin{frame}{The Forward Algorithm}
%	\begin{enumerate}
%		\item Initialization:
%		$$ \alpha_1(j) = \pi_j b_j(o_1), \quad 1 \leq j \leq N.$$
%		\item Recursion:
%		$$\alpha_t(j) = \sum_{i=1}^N \alpha_{t-1}(i) a_{ij} b_j (o_t), \quad 1 \leq j \leq N, 1 < t <T.$$
%		\item Termination:
%		$$P(O \vert \lambda) = \sum_{i=1}^N \alpha_T(i).$$
%	\end{enumerate}
%\end{frame}


\section{The Decoding Problem and the Viberti Algorithm}
\begin{frame}{Outline}
	\tableofcontents[currentsection]
\end{frame}
\begin{frame}{The Decoding Problem }
	\begin{block}{The Decoding Problem}
		Given a HMM $\lambda = (A,B)$ and a sequence of observation $O$, find the most probable hidden state sequence $Q$.
	\end{block}
	\begin{itemize}
		\item Run the forward algorithm and compute the likelihood of the observation sequence given each hidden state sequence.
		\item Choose the hidden state sequence with the maximum observation likelihood.
		\item We can get too many hidden state sequences.
	\end{itemize}
\end{frame}

\begin{frame}{Viberti Algorithm}
	\begin{itemize}
		\item Viberti algorithm is also a dynamic programming algorithm.
		\item The probability of the most probable state sequence $q_1, \cdots, q_{t-1}$ given the model $\lambda$
		\begin{equation}
			v_t(j) = \max_{q_1\cdots q_{t-1}} P(q_1\cdots q_{t-1}, o_1\cdots o_{t-1}, q_t = j \vert \lambda).
		\end{equation}
		\item $v_t(j)$ is computed it recursively it 
		\begin{equation}
			v_t(j) = \max_i[v_{t-1}(i) a_{ij} b_j(o_t)].
		\end{equation}
		\item The opimal path can be found by tracing back the recusions that led to the maximum probability.
	\end{itemize}
\end{frame}


\begin{frame}{CG Island Example}
	\begin{itemize}
		\item In genome, the frequency of CG dinucleotide is relative low (about 1\% in human genome) because of methylation. 
		\item The resulting methylated cytosine has the tendency to further deaminate into thymine \cite{compeau2018bioinformatics}.
		\item However, mythelation is often suppressed around genes, particularly near transcriptional start sites.
		\item These areas are called CG islands, where CG appears relatively frequently compared to the rest of the genome.
		\item Identifying CG islands is useful in identifying different genes.
	\end{itemize}
\end{frame}


\begin{frame}{HMM for CG Island}
	Example 1 \cite{borodovsky2006problems}: Consider the sequence S = \textbf{GGCACTGAA} and the initial, transition, and emission probability as in the following graph
	\begin{figure}
		\centering
		\includegraphics[width = 0.7\textwidth]{example1.jpg}
		\label{fig:example1}
		\caption{Example of HMM for CG island problem: 2 hidden Markov states H (high CG content) and L (low CG content), 4 observable symbols A,T C,G.}
	\end{figure}
	We want to identify the high CG content region and low CG content region.
\end{frame} 

\begin{frame}{Viberti Algorithm for CG Island}
	 The most probable path (sequence of hidden states) is \textbf{HHHLLLLLL} and its probability is $2^{-24.49} = 4.25 \times 10^{-8}$.
	\begin{figure}
		\centering
		\includegraphics[width = 0.5\textwidth]{example1log.jpg}
		\includegraphics[width = 0.7\textwidth]{example1cal.png}
		\label{fig:example2cal}
		\caption{Viberti algorithm calculation for example 1 using $\log_2$ and back tracing to find the most probable path.}
	\end{figure}
\end{frame}

\begin{frame}{HMM for CG Island}
	Example 2: Now consider the sequence  S = \textbf{GGCA}. What is the probability that sequence is generated by this model?
	\begin{figure}
		\centering
		\includegraphics[width = 0.5\textwidth]{example1.jpg}
		\includegraphics[width = 0.7\textwidth]{example2cal1.png}
		\caption{Initialization of the forward algorithm for example 2 with the calculation of $\alpha_1(j)$.}
	\end{figure}
	
\end{frame}

\begin{frame}{Forward Algorithm for CG Island}
	The probability that the sequence S = \textbf{GGCA} was generated by the HMM is 0.0038432.
	\begin{figure}
		\centering
		\includegraphics[width = 0.5\textwidth]{example1.jpg}
		\includegraphics[width = 0.7\textwidth]{example2cal2.png}
		\includegraphics[width = 0.7\textwidth]{example2cal3.png}
		\caption{Forward algorithm calculation $\alpha_t(j) = \sum_{i=1}^N \alpha_{t-1}(i)a_{ij} b_j(o_t)$ for example 2.}
	\end{figure}
\end{frame}

\section{The Training Problem and the Baum-Welch Algorithm}
\begin{frame}{Outline}
	\tableofcontents[currentsection]
\end{frame}
\begin{frame}{The Training Problem }
	\begin{block}{The Training Problem}
		Given a sequence of observation $O$ and the set of states in the HMM, learn the HMM parameters A and B.
	\end{block}
	\begin{itemize}
		\item Baum-Welch algorithm is a special case of the Expectation-Maximization (EM) algorithm \cite{Jurafsky2009}.
		\item It is an iterative algorithm: using an initial estimate for the probabilities to compute a better estimate, and so on, iteratively improving the probabilities that it learns. 
	\end{itemize}
\end{frame}


\begin{frame}{Baum-Welch Algorithm}
	\begin{itemize}
	\item Step 1: Initialize transition probabilities A and emission probabilities B.
	\item Step 2: Calculate the probability of being state $i$ at time $t$ and state $j$ at time $t+1$, given the observation sequence and the model
	\begin{equation}
		\begin{split}
			\xi_t(i,j) & = P(q_t = i, q_{t+1} =j \vert O, \lambda) \\ 
			& =\frac{ P(q_t = i, q_{t+1} =j , O \vert \lambda)}{P(O\vert \lambda)} \\
			& = \frac{\alpha_t(i) \hat{a}_{ij}\hat{b}_j(o_{t+1}) \beta_{t+1}(j)}{P(O\vert \lambda)},
		\end{split}
	\end{equation}
	and the probability of being in state $j$ at time $t$
	\begin{equation}
		\begin{split}
			\gamma_t(j) = \frac{ P(q_t = j, O \vert \lambda)}{P(O\vert \lambda)} = \frac{\alpha_t(j)\beta_t(j)}{P(O\vert \lambda)}.
		\end{split}
	\end{equation}
	\end{itemize}
\end{frame}

\begin{frame}{Baum-Welch Algorithm}
	\begin{itemize}
		\item Step 3: Estimate transition probabilities
		\begin{equation}
			\hat{a}_{ij} = \frac{\sum_{t=1}^{T-1}\xi_t(i,j)}{\sum_{t=1}^{T-1}\sum_{k=1}^{N}\xi_t(i,k)},
		\end{equation}
		and emission probabilities
		\begin{equation}
			\hat{b}_j(v_k) = \frac{\sum_{t=1 \text{s.t} O_t = v_k}^T \gamma_t(j)}{\sum_{t=1}^{T}\gamma_t(j)}.
		\end{equation}
		\item Repeat step 2 and 3 until convergence.
	\end{itemize}
\end{frame}

\section{Variations of HMM for Biological Sequence Analysis}

\begin{frame}{Outline}
	\tableofcontents[currentsection]
\end{frame}

\begin{frame}{Profile Hidden Markov Model}
	\begin{figure}
	\centering
	\includegraphics[width = 0.8\textwidth]{profile.png}
	\caption{ Example of a profile hidden Markov model.(a) Multiple sequence alignment for constructing the profile-HMM. (b) The ungapped HMM that represents the consensus sequence of the alignment.(c) The final profile-HMM that allows insertions and deletions \cite{yoon2009hidden}.}
	\end{figure}
\end{frame}

\begin{frame}{Pair Hidden Markov Model}
	
	\begin{itemize}
		\item Pair HMM is useful for the finding sequence alignments and evaluating the significance of the aligned symbols.
		\item Three hidden states: $I_x$ emits a single unaligned symbol in the first sequence $x$, $I_z$ emits an unaligned symbol in the second sequence $z$, the state $A$ generates an aligned pair of two symbols.
	\end{itemize}
	\begin{figure}
			\begin{tikzpicture}[->,-latex,shorten >=1pt,auto,node distance=20mm,thick, state/.style={circle, draw, minimum size=1cm}]
				\definecolor{qqqqff}{rgb}{0.4,0.6,1}
				\node[state](1)[draw=qqqqff]                {$I_x$};
				\node[state](3)[below right of=1, draw=qqqqff]{$A$};
				\node[state](2)[above right of=3, draw=qqqqff]{$I_z$};
				
				\path (1) edge [loop left, double distance=1pt,swap, draw=qqqqff] node {}(1);
				\path (3) edge [loop below, double distance=1pt,swap, draw=qqqqff] node {}(3);
				\path (2) edge [loop right, double distance=1pt,swap, draw=qqqqff] node {}(2);
				
				\path (1) edge [double distance=1pt,swap, draw=qqqqff, bend right = 10] node {}(2);
				\path (2) edge [double distance=1pt,swap, draw=qqqqff, bend right = 10] node {}(3);
				\path (3) edge [double distance=1pt,swap, draw=qqqqff, bend right = 10] node {}(1);
				
				\path (2) edge [double distance=1pt,swap, draw=qqqqff, bend right = 10] node {}(1);
				\path (3) edge [double distance=1pt,swap, draw=qqqqff, bend right = 10] node {}(2);
				\path (1) edge [double distance=1pt,swap, draw=qqqqff, bend right = 10] node {}(3);
			\end{tikzpicture}
	\end{figure}
	\begin{figure}
		\includegraphics[width = 0.45\textwidth]{pair.png}
	\end{figure}
\end{frame}

\begin{frame}{References}
	\bibliographystyle{abbrv}
	\bibliography{refs}
\end{frame}

\end{document}